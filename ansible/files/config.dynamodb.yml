source:
  type: dynamodb
  table: partner_user_id_mappings
  # if using DynamoDB local - specify endpoint with http:// prefix
  # Otherwise, if using aws configure, leave commented out and it will use that
  #endpoint:
  #  host: http://dynamo-local-db-source
  #  port: 8000
  #credentials:
  #  accessKey: empty
  #  secretKey: empty
  ##############################################################################
  # scan segments is how many work chunks will be created to scan the source.
  # A good ROT is to divide source table bytes, taken from describe_table
  # command, and divide by 128MB.  Roundup, and use that for both 
  # scanSegments and maxMapTasks.
  ##############################################################################
  scanSegments: 344
  # If DynamoDB is using throughput limits, you can specify a readThroughput limit
  # or a throughputReadPercent.
  #readThroughput: 1000
  #throughputReadPercent: 0.5
  # MaxMapTasks will be the number of partitions work will be broken up into.
  # Can set it equal to scanSegments.  Or to allow multiple concurrent scans by
  # a single task, set scanSegments to some multiple of maxMapTasks.
  maxMapTasks: 344

target:
  type: dynamodb
  table: partner_user_id_mappings
  # For scyllaDB - Alternator target, you need to specify endpoint URL.
  # If you have multiple workers, you could use /etc/hosts, and add 
  # an entry to each worker - such that each worker node connects to a 
  # different scylla node.  Then use the "scylla" hostname here.
  # You could alternately use the Alternator DNS load balancer, or
  # The easiest and least performant option is just point to a single nodes IP.
  # YOUR-SCYLLA-NODE0-IP scylla
  endpoint:
    host: http://node-0.aws-us-east-1.40de4a9e376f241a688c.clusters.scylla.cloud:8000
    port: 8000
  credentials:
    accessKey: empty
    secretKey: empty
  streamChanges: false
  skipInitialSnapshotTransfer: false

savepoints:
  # Where should savepoint configurations be stored? This is a path on the host running
  # the Spark driver - usually the Spark master.
  path: ./app/savepoints
  # Interval in which savepoints will be created
  intervalSeconds: 300

renames: []

validation:
  # Should WRITETIMEs and TTLs be compared?
  compareTimestamps: false
  # What difference should we allow between TTLs?
  ttlToleranceMillis: 60000
  # What difference should we allow between WRITETIMEs?
  writetimeToleranceMillis: 1000
  # How many differences to fetch and print
  failuresToFetch: 100
  # What difference should we allow between floating point numbers?
  floatingPointTolerance: 0.001
  # What difference in ms should we allow between timestamps?
  timestampMsTolerance: 0
