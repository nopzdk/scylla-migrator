# Example for loading from DynamoDB:
source:
  type: dynamodb
  table: partner_user_id_mappings
  # Optional - load from a custom endpoint:
  # endpoint:
    # Specify the hostname without a protocol
    # host: <host>
    # port: <port>

  # Optional - specify the region:
  region: us-east-1

   # Optional - static credentials:
   #  credentials:
    #    accessKey: ASIAZFNJXZP3HIUTGLGM
    #    secretKey: YrbVhPcvH5RNyBssL3UpbOkxpbA9sfDdZPQWE7vR
    #    sessionToken: FwoGZXIvYXdzEOP//////////wEaDCx7Mi/88t3afuud6yL/AUKlsU69NUJh6xfVnwwoyfRbwqz3B13QcZjCNOsoSdLaf1dRqPCvEAVinnhF5K67cQpBT1wgVrI/GJO9DoFIzfF10kf+LRxwWjz7ydOBrDoGbjV7urzMpictMyREo6e4OsoQ/WtHhT9ddNCDXI4dnFdyw5xEKhgbF3cDoRyLyqWKIncymDAJV5Qk13w2L6eeeBg/ew6wCKmwsZwlPaSGcWfujV+cJMF7jCHwoAJdWG5S52nJOF8b474LR1Nd6BOV1IDWORsxCJhjSWjVZPXXcLMP7+08BofKj5qXP1XtvnpdA3uTR3REaYdk7n8Cy1jK0c+VA0aXtHdkIgpbocsCMSiok+C6BjIr7vgX2wSKq0NIEkvozFjWVnDFgJlyV4qZ1plwfbBggPlxcRRunnWxw8kbCg==

    # Optional - assume role, see https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html
    #  assumeRole:
    #  arn: arn:aws:iam::630105558006:role/DevOpsAccessRole
      # Optional - the session name to use. If not set, we use 'scylla-migrator'
      #  sessionName: <sessionName>

  # below controls split factor
  scanSegments: 44000

  # throttling settings, set based on your capacity (or wanted capacity)
  readThroughput: 1000

  # The value of dynamodb.throughput.read.percent can be between 0.1 and 1.5, inclusively.
  # 0.5 represents the default read rate, meaning that the job will attempt to consume half of the read capacity of the table.
  # If you increase the value above 0.5, spark will increase the request rate; decreasing the value below 0.5 decreases the read request rate.
  # (The actual read rate will vary, depending on factors such as whether there is a uniform key distribution in the DynamoDB table.)
  throughputReadPercent: 1.5

  # how many tasks per executor?
  maxMapTasks: 1

# Example for loading from a DynamoDB S3 export (see https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/S3DataExport.Output.html)
# source:
#   type: dynamodb-s3-export
#   bucket: <bucket-name>
#   # Key of the `manifest-summary.json` object in the bucket
#   manifestKey: <manifest-summary-key>
#   # Key schema and attribute definitions, see https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TableCreationParameters.html
#   tableDescription:
#     attributeDefinitions:
#       - name: <attribute-name>
#         type: <attribute-type> (see https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_AttributeDefinition.html)
#       - ...
#     keySchema:
#       - name: <key-name>
#         type: <key-type> (see https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_KeySchemaElement.html)
#       - ...
#
#   # Optional - load from a custom endpoint:
#   endpoint:
#     # Specify the hostname without a protocol
#     host: <host>
#     port: <port>
#   # Optional - specify the region:
#   region: <region>
#
#   # Optional - static credentials:
#   credentials:
#     accessKey: <user>
#     secretKey: <pass>
#     # Optional - assume role, see https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html
#     assumeRole:
#       arn: <roleArn>
#       # Optional - the session name to use. If not set, we use 'scylla-migrator'
#       sessionName: <roleSessionName>
#
#   # Optional - use path-style access in S3 (see https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html)
#   usePathStyleAccess: true

# Example for loading into a DynamoDB target (for example, Scylla's Alternator):
target:
  type: dynamodb
  table: partner_user_id_mappings
  # Optional - write to a custom endpoint:
  endpoint:
    # If writing to Scylla Alternator, prefix the hostname with 'http://'.
    host: http://node-0.aws-us-east-1.282abaf2407d5eaec7c3.clusters.scylla.cloud
    port: 8000

  # Optional - specify the region:
  region: us-east-1

  # Optional - static credentials:
  credentials:
    accessKey: scylla
    secretKey: scylla
    # Optional - assume role, see https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html
    # assumeRole:
      # arn: <roleArn>
      # Optional - the session name to use. If not set, we use 'scylla-migrator'
      # sessionName: <roleSessionName>

  # throttling settings, set based on your write capacity units (or wanted capacity)
  writeThroughput: 1

  # The value of dynamodb.throughput.write.percent can be between 0.1 and 1.5, inclusively.
  # 0.5 represents the default write rate, meaning that the job will attempt to consume half of the write capacity of the table.
  # If you increase the value above 0.5, spark will increase the request rate; decreasing the value below 0.5 decreases the write request rate.
  # (The actual write rate will vary, depending on factors such as whether there is a uniform key distribution in the DynamoDB table.)
  throughputWritePercent: 1.0

  # When transferring DynamoDB sources to DynamoDB targets (such as other DynamoDB tables or Alternator tables),
  # the migrator supports transferring live changes occuring on the source table after transferring an initial
  # snapshot. This is done using DynamoDB streams and incurs additional charges due to the Kinesis streams created.
  # Enable this flag to transfer live changes after transferring an initial snapshot. The migrator will continue
  # replicating changes endlessly; it must be stopped manually.
  #
  # NOTE: For the migration to be performed losslessly, the initial snapshot transfer must complete within 24 hours.
  # Otherwise, some captured changes may be lost due to the retention period of the table's stream.
  #
  # NOTE2: The migrator does not currently delete the created Dynamo stream. Delete it manually after ending the
  # migrator run.
  streamChanges: false

  # Optional - when streamChanges is true, skip the initial snapshot transfer and only stream changes.
  # This setting is ignored if streamChanges is false.
  skipInitialSnapshotTransfer: false

# Savepoints are configuration files (like this one), saved by the migrator as it
# runs. Their purpose is to skip token ranges that have already been copied. This
# configuration only applies when copying from Cassandra/Scylla.
savepoints:
  # Where should savepoint configurations be stored? This is a path on the host running
  # the Spark driver - usually the Spark master.
  path: /app/savepoints
  # Interval in which savepoints will be created
  intervalSeconds: 300

# Optional - Column renaming configuration. If you'd like to rename any columns, specify them like so:
# - from: source_column_name
#   to: dest_column_name
# renames: []

# Optional - Which token ranges to skip. You shouldn't need to fill this in normally; the migrator will
# create a savepoint file with this filled.
# skipTokenRanges: []

# Optional - Which scan segments to skip. You shouldnâ€™t need to fill this in normally; the migrator will
# create a savepoint file with this filled.
# skipSegments: []

# Configuration section for running the validator. The validator is run manually (see documentation).
# Mandatory if the application is executed in validation mode.
# validation:
#   # Should WRITETIMEs and TTLs be compared?
#   compareTimestamps: true
#   # What difference should we allow between TTLs?
#   ttlToleranceMillis: 60000
#   # What difference should we allow between WRITETIMEs?
#   writetimeToleranceMillis: 1000
#   # How many differences to fetch and print
#   failuresToFetch: 100
#   # What difference should we allow between floating point numbers?
#   floatingPointTolerance: 0.001
#   # What difference in ms should we allow between timestamps?
#   timestampMsTolerance: 0

